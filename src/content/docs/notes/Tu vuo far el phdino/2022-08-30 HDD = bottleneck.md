---
title: 2022-08-30 HDD = bottleneck
editUrl: false
---

## Cosa ho fatto

### Novit√† relative a OC Meta

* Il processo su oc-ficlit √® diventato troppo lento. Ho scoperto che il collo di bottiglia √® l‚ÄôHDD, perch√© facendo proseguire il lavoro al mio PC su SSD la velocit√† √® aumentata di ordini di magnitudo (M.2 Nvme PCIe 4.0, 5000 MB/s in lettura e 4400 MB/s in scrittura).
  * Per sicurezza, ho ripetuto il test sempre sul mio PC su HDD e ho confermato che il problema √® l‚ÄôHDD.
  * Nella [documentazione](https://github.com/blazegraph/database/wiki/IOOptimization) di Blazegraph si legge:

    > Blazegraph depends on¬†**fast**¬†**disk access**. Low-latency, and high IOPs disk is key to high performance. **SSD offers a 10x performance** **boost** for the Journal in the embedded, standalone, and highly available replication cluster deployment modes when compared to SAS or SATA disks. In addition, SATA disks are unable to reorder writes. This can cause a marked decrease in write throughput as the size of the backing file grows over time. Since 1.3.x, we have introduced a cache compaction algorithm into the Journal that significantly reduces IOs associated with large write transactions. This makes SATA more viable, but the performance gap to SSD is still an order of magnitude for query. SAS disks can reorder read and write operations, but still have a large rotational latency and lower seek times when compared to SSD. For enterprise class deployments, you can use either PCIe flash disk or SSD in combination with the HAJournalServer (high availability). AWS now offers an SSD backed instance that is a good match for Blazegraph.\*\*\*\*
  * Utilizzare hard disk meccanici per un triplestore colossale come OC Meta render√† molto pi√π lente anche le API. Finch√© il triplestore √® piccolo non si notano differenze tra SSD e HDD, ma le differenze aumentano mano a mano che aumentano le pagine del file journal.
  * Come compromesso, si potrebbe mettere solo il triplestore su SSD e il resto dei file su HDD. Al momento, dopo aver processato il 20% dei file, il file journal pesa 100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       GB. Chiaramente l‚ÄôSSD da 439G attuale su oc-ficlit non va bene, bisognerebbe acquistarne uno nuovo, possibilmente uno che vada su linea PCIe per la maggior velocit√† possibile.
    * Per venire incontro a questa esigenza, ho inserito un parametro di configurazione opzionale che permette di scegliere la cartella di output contenente i file rdf, dato che √® di gran lunga la pi√π pesante e ha senso tenerla su HDD.
    * In questo momento il processo sta proseguendo sul mio PC con triplestore su SSD e JSON su HDD. In questo modo, il processo √® molto pi√π lento che avendo tutto su SSD, ma comunque molto pi√π veloce che avendo tutto su HDD.
* Risolto un bug per cui il Curator crashava se un metaid era stato inserito nel dizionario delle entit√† conflittuali e non in quello delle entit√† non conflittuali. Dato che non ho trovato alcun vantaggio nel distinguere i due dizionari, ho eliminato il dizionario delle entit√† conflittuali e unificato la gestione delle due tipologie di entit√†, conservando soltanto l‚Äôaggiornamento dei log.

### Novit√† relative all‚Äôarticolo per JASIST

<aside>
üìå Non riporto qui tutte le novit√†, perch√© sono troppe e si possono gi√† vedere nella nuova versione dell‚Äôarticolo e nella lettera di risposta ai revisori

</aside>

* Ho gestito il 70% dei commenti dei revisori (modifiche all‚Äôarticolo + lettera di risposta).
* Dopo lunghe peregrinazioni sono approdato alla seguente tabella comparativa

  ![attachments/1fcd87e1adfe4cc59dd214a562773be3.png](../../../../assets/notes/attachments/1fcd87e1adfe4cc59dd214a562773be3.png)

  Questa tabella risolve numerosi problemi:

  1. **Sintetizza** quelle che originariamente erano tre tabelle distinte (con buona pace di JASIST e delle sue 7000 parole massime):
     1. Quella su come rappresentare la provenance senza violare RDF e SPARQL
     2. Quella sulle politiche di archiviazione
     3. Quella sulle tipologie di query
  2. Evidenzia la vera innovazione di time-agnostic-library, ovvero la sua **genericit√†**. Infatti, da ulteriori letture era emerso che time-agnostic-library non fosse l‚Äôunico software a consentire tutte le query live, caratteristica comune anche a tutti i sistemi ispirati a Git (R\&WBase, R43ples e Quit Store)
  3. √à **coerente** con altre due comparative presenti in letteratura, che non conoscevo e che i revisori mi hanno chiesto di citare ([10.3233/SW-210434](http://doi.org/10.3233/SW-210434) e [10.1016/j.websem.2018.08.001](https://doi.org/10.1016/j.websem.2018.08.001)).
  4. Adotta una definizione pi√π precisa di ‚Äú**live**‚Äù, per rispondere a un revisore che aveva obiettato che time-agnostic-library non √® live perch√© usa dei triplestore.
  5. Aggiunge alla comparativa **R43ples**, **TailR**, **Dydra**, **Quit Store** e **RDF-TX**.
  6. Aggiunge la politica di archiviazione **fragment-based (FB)**, ovvero salvare soltanto ci√≤ che cambia (non i cambiamenti), a diversi livelli di granularit√† a seconda dei requisiti (grafi, sottografi, entit√†).
  7. **Esplicita** nella revisione della letteratura quali sono i **limiti** della **letteratura**, come chiesto da Reviewer #3.
* Sto aggiornando il **codice** dei **benchmark**
  * Per calcolare la baseline per ciascuna query, ovvero tempo di esecuzione sullo snapshot corrente e **numero di snapshot**.
    * Materializzazione di tutte le versioni di un‚Äôentit√†. Il numero di snapshot corrisponde al numero di snapshot dell‚Äôentit√†
    * Materializzazione di un intervallo della storia di un‚Äôentit√†. Il numero di snapshot corrisponde al numero di snapshot successivi all‚Äôintervallo pi√π quelli compresi nell‚Äôintervallo.
    * cross-version query. Il numero di snapshot corrisponde alla somma degli snapshot di tutte le entit√† soggetto \*\*\*\*convolte nella query.
    * single-version query. Il numero di snapshot corrisponde alla somma degli snapshot di tutte le entit√† soggetto \*\*\*\*coinvolte nella query, solo se compresi o successivi all‚Äôintervallo temporale considerato.
    * Per quanto riguarda le query sui delta il numero di snapshot non √® rilevante, perch√© a essere processati non sono gli snapshot, ma le entit√† al fine di trovare gli snapshot, quindi la prospettiva √® ribaltata. A pesare √® quindi il numero di entit√† coinvolto nella query.
* Aggiornando il codice, mi sono accorto di alcuni **comportamenti indesiderati** in **time-agnostic-library**:
  * Le query **single-version** recuperavano dalla cache l‚Äô**intera storia** delle entit√† anzich√© solo quella entro l‚Äôintervallo considerato.
  * Non c‚Äô√® ragione di processare gli URI corrispondenti ai **tipi** nelle query.
  * Non c\`√® ragione di esplicitare variabili che sono **punti morti**, ovvero che non si trovano come soggetto in un altro triple pattern all‚Äôinterno della query.
  * Le **informazioni** sull\*\*‚Äôintervallo\*\* in **cache** vengono ora **eliminate** quando tutta la storia dell‚Äôentit√† viene salvata in cache.
  * Data la necessit√† di calcolare il numero di snapshot coinvolti in una **single-version query**, ho aggiunto la **version query** secondo la classificazione dei query atoms (ritorna il risultato su quella versione pi√π informazioni sulle altre versioni esistenti). Alcuni **caveat**:
    * Se l‚Äôutente chiede informazioni su altri snapshot oltre a quelli puntati dalla sua query non pu√≤ avvalersi contemporaneamente della **cache**, perch√© la cache non contiene quelle informazioni per pesare il meno possibile.
    * Se l‚Äôutente chiede informazioni su altri snapshot ma la sua query √® su **tutte le versioni**, tali informazioni sono un **insieme vuoto**.

## Domande

### Domande relative a OC Meta

* Qual √® la differenza tra bigdata.jar e blazegraph.jar?

### Domande relative a *Performing live time-traversal queries on RDF datasets*

* **Reviewer #3** ha chiesto spiegazioni sul perch√© **RDF**\* sia stato classificato come **non scalabile**. Gli ho risposto che questa categorizzazione √® stata tratta da ([Sikos & Philp, 2020](https://doi.org/10.1007/s41019-020-00118-0)), dove ‚Äúscalabile‚Äù √® definito come ‚Äúnon causa di **esplosione di triple**‚Äù e dove RDF\* √® considerato non scalabile, senza per√≤ spiegare perch√©. Effettivamente, neanch‚Äôio capisco perch√© RDF\* non dovrebbe essere scalabile.
* Non ricordo se l‚ÄôOCDM consenta i blank nodes all‚Äôinterno delle query SPARQL di update. Direi di no.

### Altro

* La TPDLlica email
* OC Ontology su [FAIRSharing.org](http://FAIRSharing.org). Il DOI?
