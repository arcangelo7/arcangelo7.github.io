---
title: "2022-01-13"
editUrl: false
---

## Cosa ho fatto

* Ho verificato in che modo [WCW](https://github.com/opencitations/wcw) gestisce gli id temporanei delle citazioni, la loro deduplicazione e infine riconciliazione con un dataset preesistente. Sarà utile per il progetto OUTCITE, ma ho bisogno di più informazioni circa l’input e l’output del processo per capire bene dove guardare.
  1. [run\_process\_citations](https://github.com/opencitations/wcw/blob/main/Converter/run_process_citations.py) in Converter, a partire dai CSV con le citazioni, genera un primo dataset conforme a OCDM privo di duplicati.
  2. [oc\_graphenricher](https://github.com/opencitations/wcw/blob/main/Enricher/run_process.py) viene utilizzato per cercare nuovi identificatori, e InstanceMatching elimina i duplicati dal Graphset risultante.
  3. [Pusher.Reconciliator](https://github.com/opencitations/wcw/blob/main/Pusher/reconciliator.py), e in particolare la funzione reconciliate, effettua la riconciliazione tra generiche entità di OCDM sulla base della lista degli id associati. Tuttavia, al momento l’intero processo è specifico per Wikidata e andrebbe riadattato.

* [run\_preprocess](https://github.com/opencitations/meta/blob/master/plugins/crossref/crossrefProcessing.py), ovvero CrossrefProcessing, funziona ora passando una cartella di file json, una cartella di file json.gz o un archivio tar.gz. Ciò avviene grazie al nuovo modulo [jsonmanager.py](https://github.com/opencitations/meta/blob/master/lib/jsonmanager.py), che contiene le funzioni get\_all\_files e load\_json mutuate da [COCI](https://github.com/opencitations/index/blob/master/index/coci/glob.py) ed estese.

* Aggiunto un nuovo argomento —low-memory a [index\_orcid\_doi](https://github.com/opencitations/meta/blob/master/plugins/orcid/index_orcid_doi.py). Se specificato, i CSV già generati vengono caricati riga per riga fino a un threshold, processati e poi scartati per liberare spazio sulla RAM.

* Ho modificato la directory della repository. Ora tutti i plugin (Orcid, Crossref, COCI) sono nella cartella plugins. Gli script da lanciare sono invece nella cartella run. Dato che al loro nome è stata tolta la parola ‘run’, per lanciarli si usa il comando:

  ```bash
  python -m meta.run.meta_process ...
  python -m meta.run.crossref_process ...
  python -m meta.run.orcid_process ...
  python -m meta.run.coci_process ...
  ```

* I prossimi due punti sono molti prolissi, li riassumo in quattro parole: ho testato tutto Meta.

* Ho testato tutto \*\*\*\*il modulo **curator** di Meta:
  * id\_worker, ovvero il deduplication decision tree, nei seguenti casi:
    * L’entità è nuova e ci sono id (#1) ✅
    * L’entità è nuova e **non** ci sono id (#1) ✅
    * L’id dell’entità esiste già sul triplestore (#2) ✅
    * Il MetaID dell’entità esiste già sul triplestore (#2) ✅
    * Il MetaID e l’id dell’entità esistono già sul triplestore (#2) ✅
    * L’id dell’entità esiste già sul triplestore, ma è associato a un MetaID diverso (#3) ✅
    * Ci sono più id: uno esiste già nel CSV ed è associato a una sola entità temporanea. L’altro id, invece, esiste già sul triplestore ed è associato a una singola entità (#4) ❌
      * Risolto un bug per cui il titolo nel CSV aveva la precedenza su quello presente nel triplestore.
    * L’id dell’entità esiste già nel CSV ed è associato a una sola entità temporanea (#5) ✅
    * L’id dell’entità esiste già nel CSV ed è associato a una sola entità con MetaID ✅
    * Conflitto:
      * Non c’é MetaID, ma c’è un ID che evidenzia un conflitto tra due entità sul triplestore, poiché entrambe puntano al medesimo identificatore:
        * Le due entità sono di tipo fabio:Expression ✅
        * Le due entità sono di tipo foaf:Agent ✅
      * Non c’é MetaID, ma c’è un ID che evidenzia un conflitto tra due entità precedentemente recuperate dal triplestore (`local_match['existing']`), poiché entrambe puntano al medesimo identificatore ✅
      * Ci sono due ID, uno è già nel CSV associato a un’entità con MetaID, l’altro no. Quest’ultimo evidenzia un conflitto sul triplestore, poiché ad esso punta più di un’entità. ✅
      * Ci sono due ID, uno è già nel CSV associato a un’entità temporanea, l’altro no. Quest’ultimo evidenzia un conflitto sul triplestore, poiché ad esso punta più di un’entità. ✅
  * equalizer ✅
  * clean\_id ❌
    * Risolto un bug per cui il titolo non veniva correttamente aggiornato nella riga del CSV
  * check\_equality ❌
    * Risolto un bug per cui il merge avveniva solo per il primo duplicato, mentre tutti i successivi venivano ignorati.
    * Risolto un bug per cui tutti i log venivano associati alla penultima riga del CSV.
  * clean\_vvi, nei seguenti casi:
    * Tutti i dati su venue, volume e issue esistono già sul triplestore. Essi devono solo essere recuperati e riorganizzati correttamente. ✅
    * La riga del CSV è un articolo e contiene una nuova rivista. ✅
    * La riga del CSV è un articolo e contiene una rivista esistente, ma un nuovo volume e un nuovo issue. ✅
  * clean\_ra, nei seguenti casi:
    * Un agente responsabile è nel triplestore, l’altro no. ✅
    * Tutti gli agenti responsabili sono ignoti. ✅
    * L’identificatore della risorsa bibliografica è un MetaID. ✅
    * L’identificatore della risorsa bibliografica è un wannabe ID. ✅
    * Un agente responsabile ha due id, l’altro uno. ✅
    * Uno degli agenti responsabili non ha id ma è riconducibile a uno già presente nel triplestore poiché omonimo di un agente responsabile della stessa risorsa bibliografica. ✅
  * meta\_maker ✅
  * enrich ✅

* Per quanto riguarda il modulo **creator**, sono soddisfatto dei 10 test già presenti, poiché creator si limita ad applicare oc\_ocdm all’output del curator. Non vedo particolari complessità, se non quelle legate al funzionamento di oc\_ocdm e testate specificamente su oc\_ocdm. Mi sono quindi limitato ad apportare miglioramenti stilistici.
  * id\_creator: ho usato la reflection per eliminare ripetizioni e spostare in un unico punto la definizione degli schemi

    ```python
    # PRIMA
    # [...]
    if identifier.startswith("doi"):
        identifier = identifier.replace("doi:", "")
        res = self.br_index['doi'][identifier]
        url = URIRef(self.url + "id/" + res)
        new_id = self.setgraph.add_id(resp_agent, source=self.src, res=url)
        new_id.create_doi(identifier)

    elif identifier.startswith("issn"):
        identifier = identifier.replace("issn:", "")
        res = self.br_index['issn'][identifier]
        url = URIRef(self.url + "id/" + res)
        new_id = self.setgraph.add_id(resp_agent, source=self.src, res=url)
        new_id.create_issn(identifier)

    elif identifier.startswith("isbn"):
        identifier = identifier.replace("isbn:", "")
        res = self.br_index['isbn'][identifier]
        url = URIRef(self.url + "id/" + res)
        new_id = self.setgraph.add_id(resp_agent, source=self.src, res=url)
        new_id.create_isbn(identifier
    # [...]

    # DOPO
    # [...]
    for br_id_schema in self.br_id_schemas:
    		if identifier.startswith(br_id_schema):
    				identifier = identifier.replace(f'{br_id_schema}:', '')
    				res = self.br_index[br_id_schema][identifier]
    				url = URIRef(self.url + 'id/' + res)
    				new_id = self.setgraph.add_id(resp_agent, source=self.src, res=url)
    				getattr(new_id, f'create_{br_id_schema}')(identifier)
    # [...]
    ```

  * indexer\_id: stesso concetto di id\_creator

    ```python
    # PRIMA
    def indexer_id(csv_index):
    		index = dict()
    		index['crossref'] = dict()
    		index["doi"] = dict()
    		index["issn"] = dict()
    		index["isbn"] = dict()
    		index["orcid"] = dict()
    		index["pmid"] = dict()
    		index['pmcid'] = dict()
    		index['url'] = dict()
    		index['viaf'] = dict()
    		index['wikidata'] = dict()
    		index['wikipedia'] = dict()
    		
    		for row in csv_index:
    		    if row["id"].startswith("crossref"):
    		        identifier = row["id"].replace('crossref:', '')
    		        index['crossref'][identifier] = row["meta"]
    		
    		    elif row["id"].startswith("doi"):
    		        identifier = row["id"].replace('doi:', '')
    		        index['doi'][identifier] = row["meta"]
    	# [...]

    # DOPO
    def indexer_id(self, csv_index):
    		index = dict()
    		for schema in self.schemas:
    		    index[schema] = dict()
    		for row in csv_index:
    		    for schema in self.schemas:
    		        if row['id'].startswith(schema):
    		            identifier = row['id'].replace(f'{schema}:', '')
    		            index[schema][identifier] = row['meta']
    		return index
    ```

## Domande

* Esiste un ricercatore di nome **Stephen McSorley** ([https://orcid.org/0000-0002-5459-8445](https://orcid.org/0000-0002-5459-8445)). Meta trasforma questo nome in Mcsorley, S. Propongo di inserire una nuova regola che lasci maiuscole le maiuscole all’interno di un nome nel caso in cui il nome non sia tutto maiuscolo. In altre parole, applicherei la stessa regola di normalizzazione dei titoli anche ad autori, editor e publisher (e.g. OECD). Cosa ne pensi? È d'accordo
* Indipendentemente dal separatore per gli id usato in input (spazio, punto e virgola), Meta restituisce sempre in output gli id separati da spazio. Sei d’accordo con questo comportamento? È d'accordo
* L’implementazione dell’albero decisionale di deduplicazione controlla numerose volte se ci sia più di un’entità associata allo stesso id sul triplestore. In tal caso, solleva un conflitto. Tuttavia, questa eventualità mi sembra impossibile se il triplestore viene generato unicamente tramite il software di Meta, a meno di un bug. Sei d’accordo? Sì, paranoid programming
* L’implementazione di clean\_vvi prevede la possibilità che una riga del CSV contenga una risorsa di tipo ‘journal\_volume’ o ‘journal\_issue’. In quel caso, quale campo dovrebbe contenere il numero? ‘title’ o ‘volume’/’issue’? Inoltre, campi come ‘author’ e ‘page’ dovrebbero essere sempre vuoti, giusto?
* L’archivio tar.gz contenente il dump di Crossref che hai condiviso con me tramite Dropbox è danneggiato.
* Esiste la possibilità che sul triplestore si vengano a creare due identificatori con lo stesso valore letterale? Se sì, meta dovrebbe occuparsi di deduplicarli? No
* In caso di merge tra entità un titolo presente sul triplestore ha sempre la precedenza rispetto a uno presente nel CSV, giusto? Da quello che ho capito, il triplestore è immodificabile, solo incrementabile.
* Per caso sai quali sono le scadenze per il dottorato di DHDK? Non riesco a trovare informazioni sul sito di Unibo. Bando esce in aprile, con chiusura fine maggio. Tra giugno e luglio ci saranno le prove. Bisogna presentare una proposta di progetto. Poi c'è l'orale, lo scritto in precedenza non è stato fatto. In teoria tre prove: proposta di progetto, scritto e orale. Presenta anche in altre due parti: Data Science (Fabio) e Informatica (Paolo Ciancarlini).
* Ti ho mandato una mail sul modulo da compilare per ottenere le credenziali della vpn, l’hai ricevuta?
